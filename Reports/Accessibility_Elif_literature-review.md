
---

### Research Analysis Log: [Insert Test Type, e.g., Mutation Testing]

**Researcher:** ELİF ATAŞ
**Date:** 04.11.2025

## 1. Executive Synthesis

LLM-assisted test generation aims to automate the creation of readable and reliable test code, addressing one of QA’s main challenges—manual, time-consuming test writing. By using large language models to produce context-aware tests, this approach enhances software quality, maintainability, and accessibility.

Research shows that AI-generated code is generally more readable and well-documented (Fawareh et al., 2024), and integrating LLMs with static analysis improves precision while reducing false positives (Li et al., 2024). However, studies also highlight the unpredictability of LLMs and the limitations of existing automated tools in detecting complex, accessibility-related flaws (Kodithuwakku & Wickramaarachchi, 2025). Current benchmarks and LLMOps research emphasize the need for reliable validation and continuous monitoring.

These findings shape LLM-QA’s direction: it should focus on generating precise, readable, and validated test code integrated into CI/CD pipelines. Combining insights from code quality, static analysis, and accessibility studies, LLM-QA positions itself as a practical and timely solution that enhances both defect detection and developer accessibility.

## 2. Annotated Bibliography & Analysis


### Article 1: Investigates the Impact of AI-generated Code Tools on Software Readability Code Quality Factor

**Full Citation (APA 7th Style):**
Fawareh, H., Al-Shdaifat, H. M., Khouj, M., Al-Refai, M., & Fawareh, F. A. (2024). Investigates the Impact of AI-generated Code Tools on Software Readability Code Quality Factor. 2024 25th International Arab Conference on Information Technology (ACIT). IEEE. DOI: 10.1109/ACIT62805.2024.10876897

**In-Text Citation Example:**
(Fawareh et al., 2024)

**Summary of Contribution:**
This study investigates the software performance of code generated by AI tools like ChatGPT, focusing specifically on readability, complexity, and documentation metrics. The authors used key metrics such as Cyclomatic Complexity (CC), Lines of Code (LOC), and comment density to compare AI-generated code with open-source human code on GitHub for three tasks of varying difficulty. The results indicate that AI-generated code, particularly for more complex tasks, exhibits better readability than human code due to its high comment density and clear variable names.

**Key Findings & Quotations:**
Artificial intelligence (AI)-generated code has more clearly demonstrated a "higher focus on documentation" compared to human-generated open-source code (Fawareh et al., 2024).

In "complex tasks," in particular, AI-generated code exhibited improved readability due to higher comment density and the use of clear variable names. For example, on the most difficult task, the ChatGPT code showed a Comment Density of 21.88%, while the GitHub code remained at 0% (Fawareh et al., 2024).

It has been stated that software quality is a key factor and is often evaluated based on characteristics such as "readability, maintainability, efficiency, and correctness" (Fawareh et al., 2024).

**Personal Analysis & Relevance to LLM-QA:**
This article provides foundational evidence for the most innovative claim of our project, LLM-QA. Since the output of our project is a test code, the readability and clarity of this test code are of primary importance.

Accessibility Bridge
This work proves that the code generated by the LLM naturally has a higher level of readability and documentation. This is the strongest argument that links our project to the WCAG's "Understandable" principle. For the developer, the test code that proves the error being clear and well-commented reduces the cognitive load, meaning it increases "developer accessibility."

Feature Idea
We should add constraints to the prompt structure (request) we use when asking the LLM for test code, which mandates the metrics used in this study (high comment density, clear variable/function names). This allows us to further optimize the inherent quality of the LLM.

Limitation
The study did not examine the specific quality of AI-generated test code; it examined general code quality. Therefore, we will need additional references for the effectiveness (bug coverage) and reliability of the unit tests produced by LLM-QA.

### Article 2: Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach

**Full Citation (APA 7th Style):**
Li, H., Hao, Y., Zhai, Y., & Qian, Z. (2024). Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach. Proceedings of the ACM on Programming Languages, 8(PLDI), 1–25. DOI: 10.1145/3658249

**In-Text Citation Example:**
(Li et al., 2024)

**Summary of Contribution:**
This work presents an innovative framework called LLIFT, which combines Large Language Models (LLMs) with traditional Static Code Analysis (SCA). The main goal is to reduce the primary problem of SCA in large and complex codebases: low precision and, consequently, a high False Positive rate. LLIFT leverages the sophisticated reasoning capability of LLMs to detect complex bug types (e.g., UBI) and filters warnings coming from SCA. Real-world evaluations show that this integrated approach has been successful in detecting previously unknown critical bugs that traditional tools missed, establishing a strong direction for future research in this field.

**Key Findings & Quotations:**
It has been emphasized that the core obstacle to the practical use of Static Analysis is its "low precision," which produces a high number of false positives (False Positive), especially in complex codebases.

It has been confirmed that LLMs offer a strong potential moving forward in the field of "complex program analysis," compared to previous works that only focused on simple code repair.

The LLIFT framework states that it uses "carefully designed procedures" to overcome challenges such as the unpredictable nature of the LLM, comprehensive codebases, and bug-specific modeling. This indicates that making the LLM effective requires focusing not only on the model itself but also on the surrounding engineering processes.

**Personal Analysis & Relevance to LLM-QA:**
Direct Correlation (Technical Justification)
LLM-QA's main claim is to find precise, verifiable flaws via LLMs, as opposed to the vague warnings of SCA. LLIFT's success (four previously undiscovered UBI bugs) is proof of how the LLM provides this high precision. This forms the basis of our "Flaw-to-Test" approach.

Accessibility Bridge (Developer Experience)
If our SCA/LLM integration significantly reduces false positives (False Positives), the developer only encounters test code addressing genuine issues. Not having to investigate false alarms reduces the developer's cognitive load and, consequently, the accessibility of the development process. The high technical precision of our project indirectly ensures a better user (developer) experience and accessibility.

Risk Warning
The "unpredictable nature of LLMs" mentioned by the authors is a significant warning. This indicates that when generating test code from the LLM, we must use a strong validation mechanism to guarantee the code's readability and correctness. Otherwise, the test code produced by the LLM could be unreliable, which would harm developer accessibility.
### Article 3: Assessing the Limits of Automated Accessibility Testing: Insights for QA Engineers and Tool Developers

**Full Citation (APA 7th Style):**
Kodithuwakku, T., & Wickramaarachchi, D. (2025). Assessing the Limits of Automated Accessibility Testing: Insights for QA Engineers and Tool Developers. 2025 5th International Conference on Advanced Research in Computing (ICARC). IEEE. DOI: 10.1109/ICARC64760.2025.10963173
**In-Text Citation Example:**
(Kodithuwakku & Wickramaarachchi, 2025)

**Summary of Contribution:**
This research aimed to evaluate the coverage and effectiveness of seven current popular automated accessibility testing tools (Axe, Lighthouse, WAVE, etc.). The authors compared the performance of the tools across critical metrics such as True Positive (TP), False Positive (FP), and Issue Coverage Ratio (ICR) by conducting both automated and manual tests. The findings indicated that automated tools can only effectively check certain, technically simple WCAG rules, and that manual testing remains indispensable for comprehensive accessibility assurance. This provides a clear perspective for tool developers and QA engineers regarding where the limits of automation end.

**Key Findings & Quotations:**
It has been confirmed that automated tools are inadequate in detecting complex and context-dependent accessibility flaws, leading to the necessity of manual testing.

The study focused on "Issue Coverage Ratio (ICR)" and "False Positive (FP)" metrics among the metrics used to evaluate tool performance. This quantitatively revealed how many flaws automated tests can genuinely find and how many false alarms they generate.

The authors suggested that, given the limitations of these tools, QA engineers should adopt a comprehensive testing strategy that goes beyond automated tools.

**Personal Analysis & Relevance to LLM-QA:**
This article provides critical evidence to define the mission and market positioning of the LLM-QA project.

Direct Correlation (Proof of Need)
Our project's main argument is to find context-dependent, complex flaws—where existing tools (like the automated accessibility tools examined in this article) fall short—using the sophisticated reasoning capability of the LLM. This article scientifically establishes the need for a smarter tool like LLM-QA by demonstrating the limitations of automated testing (low ICR, high FP risk) with metrics.

Conceptual Bridge (Metrics)
The True Positive (TP) and False Positive (FP) metrics used in the article are the core metrics of LLIFT (SCA integration with LLM), which we examined in Article 2. This provides strong evidence that we are using the same set of quality metrics for both the technical core (bug detection) and the niche area (accessibility) of LLM-QA.

Feature Idea
LLM-QA must be designed to find flaws that automated tools miss. This article offers a list of WCAG criteria that are often overlooked by automated tests and require manual inspection. This list can serve as a starting point for a targeted strategy when prompting the LLM, such as instructing it to "specifically generate test code for these WCAG flaw patterns that typically fall into manual review." This maximizes the added value of LLM-QA.
### Article 4: Surveying the Benchmarking Landscape of Large Language Models in Code Intelligence

**Full Citation (APA 7th Style):**
Surveying the Benchmarking Landscape of Large Language Models in Code Intelligence
**In-Text Citation Example:**
(Abdollahi et al., 2025)

**Summary of Contribution:**
This article is a comprehensive study that systematically examines the evaluation landscape of Large Language Models (LLMs) in the field of Code Intelligence by reviewing 142 related articles and 156 different benchmark sets published between 2020 and 2025. The research analyzes the features, strengths, and weaknesses of benchmarks that assess the capabilities of LLMs across 32 different coding tasks, including code generation, debugging, and software testing. The findings illustrate current trends in the field, dataset construction methods, and alignment with real-world challenges, thus guiding how LLM-based software engineering tools should be developed and tested.

**Key Findings & Quotations:**
It was determined that Python is the most common programming language in 77% of the datasets used to evaluate LLMs, and GitHub is the source for 46% of the data.

It was stated that a rigorous and meaningful benchmarking of LLMs' capabilities in coding tasks is a fundamental necessity, and critical tasks such as "software testing" are among the tasks examined.

An "increasing trend" in the publication of new benchmark datasets for code intelligence has been observed in the last three years, indicating that the research field continues to mature.

**Personal Analysis & Relevance to LLM-QA:**
This article establishes the methodological foundation and scientific validity of our LLM-QA project. It allows us to understand how our project should be measured and how its results should be interpreted.

Direct Correlation (Methodology)
Our study's focus on Python code within GitHub projects (open source) aligns perfectly with the dominant research trends in the field identified by this article (46% GitHub, 77% Python). This proves that the data collection and implementation scope of our LLM-QA project are consistent with current scientific standards.

Accessibility Bridge (Measurability)
While the first three articles show us what to do (LLM code quality, high precision, overcoming the limits of automated tools), this article shows us how to measure it. The benchmarking tasks and performance metrics mentioned in the study provide a reference framework for evaluating the effectiveness (coverage, true/false positive rates) of the test code produced by LLM-QA.

Risk Warning and Idea
The article implies that there are challenges regarding the reliability of LLMs even in benchmarking studies. This suggests that LLM-QA should not only generate test code but also automatically run and validate the generated test (CI/CD integration). The validation phase will be our primary line of defense to mitigate the risks arising from the unpredictable nature of LLMs.
### Article 5: Systematic Technical Survey on LLMOps: Lifecycle, Tools, Challenges, and Emerging Practices

**Full Citation (APA 7th Style):**
Özer, F. C. (2025). Systematic Technical Survey on LLMOps: Lifecycle, Tools, Challenges, and Emerging Practices [Master's Thesis, University of Eastern Finland]. UEF Electronic Publications.

**In-Text Citation Example:**
(Özer, 2025)

**Summary of Contribution:**
This article solidifies the feasibility and integration dimension of the LLM-QA project. It demonstrates that our project is not merely a lab experiment but a tool that can be seamlessly embedded into the modern software development environment.

Direct Correlation (CI/CD Integration)
Since LLM-QA aims to perform automated analysis in GitHub projects, full integration into the CI/CD pipeline is a fundamental requirement. This article's LLMOps framework clearly shows which technical steps (Prompt Engineering, Test Data Monitoring) we must follow when placing LLM-QA into CI/CD.

Accessibility Bridge (Reliability)
Developer Accessibility (as discussed in Articles 1 and 2) relies on the reliability of the LLM output. This article states that the way to mitigate unpredictability and drift issues in LLM output is through continuous monitoring. Continuously monitoring the test code produced by LLM-QA in the CI/CD environment, even after it verifies the flaw, increases its reliability for the developer. This is a mandatory operational step for LLM-QA to deliver on its quality assurance promise.

Feature Idea
We should consider the idea of adding a simple LLMOps Monitoring Dashboard to our project that tracks the "viability" of the LLM-generated test code (e.g., how many of the last 100 test codes produced a True Positive?). This feature would enhance both the project's academic contribution and its commercial viability.

**Key Findings & Quotations:**
It has been determined that the LLMOps lifecycle includes new, LLM-specific stages such as "Prompt Engineering" and "Reinforcement Learning with Human Feedback (RLHF)," which differentiate it from traditional MLOps.

Critical challenges in the deployment of LLM applications include "model unpredictability, data drift, and security risks." These challenges specifically require the continuous monitoring of LLM outputs (test code in our case).

The study stated that the integration of LLMs into the Continuous Integration and Continuous Delivery (CI/CD) environment requires a strong observability infrastructure to monitor the LLM's performance and guarantee the quality of its output.

**Personal Analysis & Relevance to LLM-QA:**
This article solidifies the feasibility and integration dimension of the LLM-QA project. It demonstrates that our project is not merely a lab experiment but a tool that can be seamlessly embedded into the modern software development environment.

Direct Correlation (CI/CD Integration)
Since LLM-QA aims to perform automated analysis in GitHub projects, full integration into the CI/CD pipeline is a fundamental requirement. This article's LLMOps framework clearly shows which technical steps (Prompt Engineering, Test Data Monitoring) we must follow when placing LLM-QA into CI/CD.

Accessibility Bridge (Reliability)
Developer Accessibility (as discussed in Articles 1 and 2) relies on the reliability of the LLM output. This article states that the way to mitigate unpredictability and drift issues in LLM output is through continuous monitoring. Continuously monitoring the test code produced by LLM-QA in the CI/CD environment, even after it verifies the flaw, increases its reliability for the developer. This is a mandatory operational step for LLM-QA to deliver on its quality assurance promise.

Feature Idea
We should evaluate the idea of adding a simple LLMOps Monitoring Dashboard to our project that tracks the "viability" of the LLM-generated test code (e.g., how many of the last 100 test codes produced a True Positive?). This feature would enhance both the project's academic contribution and its commercial viability.

---







-